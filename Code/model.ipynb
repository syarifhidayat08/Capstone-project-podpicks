{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install if not available yet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in d:\\kuliah stuff\\kuliah smt 6\\capstone\\product\\.venv\\lib\\site-packages (2.0.3)\n",
      "Requirement already satisfied: scikit-learn in d:\\kuliah stuff\\kuliah smt 6\\capstone\\product\\.venv\\lib\\site-packages (1.3.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in d:\\kuliah stuff\\kuliah smt 6\\capstone\\product\\.venv\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in d:\\kuliah stuff\\kuliah smt 6\\capstone\\product\\.venv\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in d:\\kuliah stuff\\kuliah smt 6\\capstone\\product\\.venv\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: numpy>=1.20.3 in d:\\kuliah stuff\\kuliah smt 6\\capstone\\product\\.venv\\lib\\site-packages (from pandas) (1.24.3)\n",
      "Requirement already satisfied: scipy>=1.5.0 in d:\\kuliah stuff\\kuliah smt 6\\capstone\\product\\.venv\\lib\\site-packages (from scikit-learn) (1.10.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in d:\\kuliah stuff\\kuliah smt 6\\capstone\\product\\.venv\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in d:\\kuliah stuff\\kuliah smt 6\\capstone\\product\\.venv\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: six>=1.5 in d:\\kuliah stuff\\kuliah smt 6\\capstone\\product\\.venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pandas scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in d:\\kuliah stuff\\kuliah smt 6\\capstone\\product\\.venv\\lib\\site-packages (4.41.2)\n",
      "Requirement already satisfied: filelock in d:\\kuliah stuff\\kuliah smt 6\\capstone\\product\\.venv\\lib\\site-packages (from transformers) (3.14.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in d:\\kuliah stuff\\kuliah smt 6\\capstone\\product\\.venv\\lib\\site-packages (from transformers) (0.23.2)\n",
      "Requirement already satisfied: numpy>=1.17 in d:\\kuliah stuff\\kuliah smt 6\\capstone\\product\\.venv\\lib\\site-packages (from transformers) (1.24.3)\n",
      "Requirement already satisfied: packaging>=20.0 in d:\\kuliah stuff\\kuliah smt 6\\capstone\\product\\.venv\\lib\\site-packages (from transformers) (24.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in d:\\kuliah stuff\\kuliah smt 6\\capstone\\product\\.venv\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in d:\\kuliah stuff\\kuliah smt 6\\capstone\\product\\.venv\\lib\\site-packages (from transformers) (2024.5.15)\n",
      "Requirement already satisfied: requests in d:\\kuliah stuff\\kuliah smt 6\\capstone\\product\\.venv\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in d:\\kuliah stuff\\kuliah smt 6\\capstone\\product\\.venv\\lib\\site-packages (from transformers) (0.19.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in d:\\kuliah stuff\\kuliah smt 6\\capstone\\product\\.venv\\lib\\site-packages (from transformers) (0.4.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in d:\\kuliah stuff\\kuliah smt 6\\capstone\\product\\.venv\\lib\\site-packages (from transformers) (4.66.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in d:\\kuliah stuff\\kuliah smt 6\\capstone\\product\\.venv\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (2024.6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in d:\\kuliah stuff\\kuliah smt 6\\capstone\\product\\.venv\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (4.5.0)\n",
      "Requirement already satisfied: colorama in d:\\kuliah stuff\\kuliah smt 6\\capstone\\product\\.venv\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\kuliah stuff\\kuliah smt 6\\capstone\\product\\.venv\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\kuliah stuff\\kuliah smt 6\\capstone\\product\\.venv\\lib\\site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\kuliah stuff\\kuliah smt 6\\capstone\\product\\.venv\\lib\\site-packages (from requests->transformers) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\kuliah stuff\\kuliah smt 6\\capstone\\product\\.venv\\lib\\site-packages (from requests->transformers) (2024.6.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Capstone\\.conda\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\syari\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import tensorflow as tf\n",
    "import transformers\n",
    "from transformers import TFBertModel, BertTokenizer\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num CPUs Available:  1\n",
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(\"Num CPUs Available: \", len(tf.config.list_physical_devices('CPU')))\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/device:GPU:0'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.test.gpu_device_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Genre</th>\n",
       "      <th>Podcast Name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>arts and entertainment</td>\n",
       "      <td>Easy Stories in English</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>arts and entertainment</td>\n",
       "      <td>Podcast Buku Kutu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>arts and entertainment</td>\n",
       "      <td>Underwood and Flinch and Other Audiobooks by M...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>arts and entertainment</td>\n",
       "      <td>Podcast Resensi Buku</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>arts and entertainment</td>\n",
       "      <td>SupremeMasterTV</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Genre                                       Podcast Name\n",
       "0  arts and entertainment                            Easy Stories in English\n",
       "1  arts and entertainment                                  Podcast Buku Kutu\n",
       "2  arts and entertainment  Underwood and Flinch and Other Audiobooks by M...\n",
       "3  arts and entertainment                               Podcast Resensi Buku\n",
       "4  arts and entertainment                                    SupremeMasterTV"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load your dataset\n",
    "data = pd.read_csv('../Data/podcasts_data.csv')\n",
    "data = data.iloc[:, :2]\n",
    "data.columns = ['Genre', 'Podcast Name']\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Separated files 'genres.csv' and 'podcasts.csv' have been created.\n"
     ]
    }
   ],
   "source": [
    "# Separate the columns into different DataFrames\n",
    "genres = data[['Genre']].drop_duplicates().reset_index(drop=True)\n",
    "podcasts = data[['Podcast Name']].drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "# Save each DataFrame to a CSV file\n",
    "genres.to_csv('genres.csv', index=False)\n",
    "podcasts.to_csv('podcasts.csv', index=False)\n",
    "\n",
    "print(\"Separated files 'genres.csv' and 'podcasts.csv' have been created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable have been called\n"
     ]
    }
   ],
   "source": [
    "genres = pd.read_csv('./genres.csv')\n",
    "podcasts = pd.read_csv('./podcasts.csv')\n",
    "\n",
    "print(\"Variable have been called\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "# Tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = TFBertModel.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BERT Embeddings\n",
    "def get_bert_embeddings_batch(text_list, tokenizer, model, batch_size):\n",
    "    batch_embeddings = []\n",
    "    for i in range(0, len(text_list), batch_size):\n",
    "        batch = text_list[i:i+batch_size]\n",
    "        inputs = tokenizer(batch, return_tensors='tf', padding=True, truncation=True)\n",
    "        outputs = model(inputs)\n",
    "        batch_embeddings.append(tf.reduce_mean(outputs.last_hidden_state, axis=1).numpy())\n",
    "    return np.concatenate(batch_embeddings)\n",
    "\n",
    "genres_embeddings = get_bert_embeddings_batch(genres['Genre'].tolist(), tokenizer, model, batch_size=128)\n",
    "podcasts_embeddings = get_bert_embeddings_batch(podcasts['Podcast Name'].tolist(), tokenizer, model, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def search(query, tokenizer, model, data_embeddings, data_labels, top_k=5):\n",
    "    query_embedding = get_bert_embeddings_batch([query], tokenizer, model, batch_size=1)\n",
    "    similarities = cosine_similarity(query_embedding, data_embeddings).flatten()\n",
    "    top_k_indices = similarities.argsort()[-top_k:][::-1]\n",
    "    return [(data_labels[idx], similarities[idx]) for idx in top_k_indices]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Podcast Name: English 102: Genre Remediation , Similarity: 0.6366\n",
      "Podcast Name: Easy Stories in English , Similarity: 0.6311\n",
      "Podcast Name: Limited Lexicon , Similarity: 0.6298\n",
      "Podcast Name: Speculative Fiction Writing Made Simple , Similarity: 0.6258\n",
      "Podcast Name: Lit In 9ja , Similarity: 0.6122\n"
     ]
    }
   ],
   "source": [
    "# Example search\n",
    "query = \"easy english\"\n",
    "results = search(query, tokenizer, model, podcasts_embeddings, podcasts['Podcast Name'].tolist())\n",
    "for name, score in results:\n",
    "    print(f\"Podcast Name: {name} , Similarity: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top Genre: hobbies\n",
      "Podcast Name: C.O.O.L. Radio, Similarity: 0.5270\n",
      "Podcast Name: Audiobooks with Subrata, Similarity: 0.5028\n",
      "Podcast Name: Hobbies, Similarity: 0.4926\n",
      "Podcast Name: Reel Friends: A Movie Podcast, Similarity: 0.4918\n",
      "Podcast Name: FO Paris Podcasts, Similarity: 0.4908\n"
     ]
    }
   ],
   "source": [
    "def search_podcasts_by_genre(genre_query, tokenizer, model, genres_embeddings, podcasts_embeddings, genres_labels, podcasts_labels, top_k=5):\n",
    "    genre_results = search(genre_query, tokenizer, model, genres_embeddings, genres_labels, top_k=1)\n",
    "    top_genre, _ = genre_results[0]\n",
    "    print(f\"Top Genre: {top_genre}\")\n",
    "\n",
    "    results = search(top_genre, tokenizer, model, podcasts_embeddings, podcasts_labels, top_k)\n",
    "    for name, score in results:\n",
    "        print(f\"Podcast Name: {name}, Similarity: {score:.4f}\")\n",
    "\n",
    "# Example search\n",
    "search_podcasts_by_genre(\"Religion\", tokenizer, model, genres_embeddings, podcasts_embeddings, genres['Genre'].tolist(), podcasts['Podcast Name'].tolist())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
