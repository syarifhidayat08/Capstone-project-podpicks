{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Langkah 1: Preprocessing Teks"
      ],
      "metadata": {
        "id": "4cQXRX-nJ93j"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "46UUBLweI2lI",
        "outputId": "2b854ecf-0801-489d-b462-b34a4c21c461"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "from sklearn.model_selection import train_test_split\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('indonesian'))\n",
        "\n",
        "# Fungsi untuk membersihkan teks\n",
        "def clean_text(text):\n",
        "    if isinstance(text, str):\n",
        "        text = re.sub(r'\\d+', '', text)  # Hapus angka\n",
        "        text = re.sub(r'\\s+', ' ', text)  # Hapus spasi berlebih\n",
        "        text = text.lower()  # Ubah teks menjadi huruf kecil\n",
        "        text = re.sub(r'[^\\w\\s]', '', text)  # Hapus tanda baca\n",
        "        text = ' '.join([word for word in text.split() if word not in stop_words])  # Hapus stopwords\n",
        "        return text\n",
        "    return ''\n",
        "\n",
        "# Load the dataset\n",
        "data = pd.read_excel('data_podcast.xlsx', sheet_name='Sheet1')\n",
        "\n",
        "# Preprocess the data\n",
        "data['combined_features'] = data['Genre'].astype(str) + ' ' + data['Podcast Name'].astype(str) + ' ' + data['Description'].astype(str) + ' ' + data['Publisher'].astype(str)\n",
        "data['combined_features'] = data['combined_features'].apply(clean_text)\n",
        "data['combined_features'].fillna('', inplace=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Langkah 2: Tokenisasi dan Pembuatan Embedding Matrix"
      ],
      "metadata": {
        "id": "HuJcMHwdKXeH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_path = 'cc.id.300.vec'\n",
        "embedding_path = '/path/to/your/embeddings/cc.id.300.vec'\n"
      ],
      "metadata": {
        "id": "Pxxx7xR0v3Jx"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "O5nJw_Lev_yw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import KeyedVectors\n",
        "\n",
        "# Tokenize the text\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(data['combined_features'])\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "\n",
        "# Convert text data to sequences\n",
        "sequences = tokenizer.texts_to_sequences(data['combined_features'])\n",
        "\n",
        "# Pad sequences to ensure uniform length\n",
        "max_sequence_length = max([len(seq) for seq in sequences])\n",
        "padded_sequences = pad_sequences(sequences, maxlen=max_sequence_length, padding='post')\n",
        "\n",
        "# Load pretrained FastText embeddings\n",
        "embedding_path = 'cc.id.300.vec'  # Path to FastText embeddings\n",
        "word_vectors = KeyedVectors.load_word2vec_format(embedding_path)\n",
        "\n",
        "# Create embedding matrix\n",
        "embedding_dim = 300\n",
        "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
        "\n",
        "for word, i in tokenizer.word_index.items():\n",
        "    if word in word_vectors:\n",
        "        embedding_matrix[i] = word_vectors[word]\n",
        "    else:\n",
        "        embedding_matrix[i] = np.random.normal(size=(embedding_dim,))\n",
        "\n",
        "# Free memory\n",
        "del word_vectors\n",
        "\n",
        "# Convert target variable to numeric\n",
        "label_column = 'Genre'\n",
        "labels = pd.get_dummies(data[label_column])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "gZTl_84yKX4o",
        "outputId": "a2617740-2685-4b74-9c8b-568ed1df0899"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'cc.id.300.vec'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-3ab03111b7df>\u001b[0m in \u001b[0;36m<cell line: 17>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# Load pretrained FastText embeddings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0membedding_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'cc.id.300.vec'\u001b[0m  \u001b[0;31m# Path to FastText embeddings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mword_vectors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKeyedVectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_word2vec_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedding_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m# Create embedding matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mload_word2vec_format\u001b[0;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype, no_header)\u001b[0m\n\u001b[1;32m   1717\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1718\u001b[0m         \"\"\"\n\u001b[0;32m-> 1719\u001b[0;31m         return _load_word2vec_format(\n\u001b[0m\u001b[1;32m   1720\u001b[0m             \u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfvocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0municode_errors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0municode_errors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1721\u001b[0m             \u001b[0mlimit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlimit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatatype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdatatype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mno_header\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mno_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36m_load_word2vec_format\u001b[0;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype, no_header, binary_chunk_size)\u001b[0m\n\u001b[1;32m   2046\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2047\u001b[0m     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"loading projection weights from %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2048\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfin\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2049\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mno_header\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2050\u001b[0m             \u001b[0;31m# deduce both vocab_size & vector_size from 1st pass over file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/smart_open/smart_open_lib.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(uri, mode, buffering, encoding, errors, newline, closefd, opener, compression, transport_params)\u001b[0m\n\u001b[1;32m    175\u001b[0m         \u001b[0mtransport_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 177\u001b[0;31m     fobj = _shortcut_open(\n\u001b[0m\u001b[1;32m    178\u001b[0m         \u001b[0muri\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m         \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/smart_open/smart_open_lib.py\u001b[0m in \u001b[0;36m_shortcut_open\u001b[0;34m(uri, mode, compression, buffering, encoding, errors, newline)\u001b[0m\n\u001b[1;32m    361\u001b[0m         \u001b[0mopen_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'errors'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 363\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_builtin_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocal_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffering\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbuffering\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mopen_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'cc.id.300.vec'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Langkah 3: Menggunakan Word Embeddings dan Model LSTM (Opsional)"
      ],
      "metadata": {
        "id": "3JYKN0YuKauj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
        "\n",
        "# Load pre-trained word embeddings for Indonesian (e.g., from FastText)\n",
        "# Here, we assume that `embedding_matrix` and `vocab_size` have been prepared\n",
        "\n",
        "# Tokenize the text\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(data['combined_features'])\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "\n",
        "# Convert text data to sequences\n",
        "sequences = tokenizer.texts_to_sequences(data['combined_features'])\n",
        "\n",
        "# Pad sequences to ensure uniform length\n",
        "max_sequence_length = max([len(seq) for seq in sequences])\n",
        "padded_sequences = pad_sequences(sequences, maxlen=max_sequence_length, padding='post')\n",
        "\n",
        "# Convert target variable to numeric\n",
        "label_column = 'Genre'\n",
        "labels = pd.get_dummies(data[label_column])\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(padded_sequences, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the LSTM model\n",
        "embedding_dim = 100\n",
        "model = Sequential([\n",
        "    Embedding(vocab_size, embedding_dim, input_length=max_sequence_length),\n",
        "    LSTM(128, return_sequences=False),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dense(labels.shape[1], activation='softmax')  # Softmax for multi-class classification\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))\n",
        "\n",
        "# Evaluate the model\n",
        "test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
        "print(\"Test Loss:\", test_loss)\n",
        "print(\"Test Accuracy:\", test_accuracy)\n",
        "\n",
        "def recommend_podcast(keyword, data, tokenizer, model, max_sequence_length):\n",
        "    keyword = clean_text(keyword)\n",
        "    keyword_sequence = tokenizer.texts_to_sequences([keyword])\n",
        "    keyword_padded = pad_sequences(keyword_sequence, maxlen=max_sequence_length, padding='post')\n",
        "    predictions = model.predict(keyword_padded)\n",
        "    sorted_indices = np.argsort(predictions, axis=1)[:, ::-1]\n",
        "    top_indices = sorted_indices[0, :5]\n",
        "    for index in top_indices:\n",
        "        print(\"Genre:\", data.iloc[index]['Genre'])\n",
        "        print(\"Podcast Name:\", data.iloc[index]['Podcast Name'])\n",
        "        print(\"Spotify URL:\", data.iloc[index]['Spotify URL'])\n",
        "        print()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vc31Z7dCKhM-",
        "outputId": "f3b1abd7-fbd9-4c98-d3c6-12c93b460481"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "55/55 [==============================] - 195s 3s/step - loss: 3.9389 - accuracy: 0.0211 - val_loss: 3.9191 - val_accuracy: 0.0205\n",
            "Epoch 2/10\n",
            "55/55 [==============================] - 192s 4s/step - loss: 3.9091 - accuracy: 0.0216 - val_loss: 3.9014 - val_accuracy: 0.0205\n",
            "Epoch 3/10\n",
            "55/55 [==============================] - 185s 3s/step - loss: 3.8918 - accuracy: 0.0182 - val_loss: 3.8936 - val_accuracy: 0.0136\n",
            "Epoch 4/10\n",
            "55/55 [==============================] - 183s 3s/step - loss: 3.8828 - accuracy: 0.0211 - val_loss: 3.8912 - val_accuracy: 0.0114\n",
            "Epoch 5/10\n",
            "55/55 [==============================] - 183s 3s/step - loss: 3.8787 - accuracy: 0.0245 - val_loss: 3.8901 - val_accuracy: 0.0136\n",
            "Epoch 6/10\n",
            "55/55 [==============================] - 174s 3s/step - loss: 3.8799 - accuracy: 0.0188 - val_loss: 3.8895 - val_accuracy: 0.0136\n",
            "Epoch 7/10\n",
            "55/55 [==============================] - 182s 3s/step - loss: 3.8776 - accuracy: 0.0176 - val_loss: 3.8862 - val_accuracy: 0.0114\n",
            "Epoch 8/10\n",
            "55/55 [==============================] - 173s 3s/step - loss: 3.8790 - accuracy: 0.0211 - val_loss: 3.8863 - val_accuracy: 0.0182\n",
            "Epoch 9/10\n",
            "55/55 [==============================] - 180s 3s/step - loss: 3.8773 - accuracy: 0.0188 - val_loss: 3.8858 - val_accuracy: 0.0205\n",
            "Epoch 10/10\n",
            "55/55 [==============================] - 179s 3s/step - loss: 3.8771 - accuracy: 0.0216 - val_loss: 3.8860 - val_accuracy: 0.0159\n",
            "14/14 [==============================] - 12s 839ms/step - loss: 3.8860 - accuracy: 0.0159\n",
            "Test Loss: 3.886016845703125\n",
            "Test Accuracy: 0.015909090638160706\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional\n",
        "\n",
        "# Define the LSTM model\n",
        "model = Sequential([\n",
        "    Embedding(vocab_size, embedding_dim, weights=[embedding_matrix], input_length=max_sequence_length, trainable=False),\n",
        "    Bidirectional(LSTM(128, return_sequences=True)),\n",
        "    Dropout(0.5),\n",
        "    LSTM(64),\n",
        "    Dropout(0.5),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(labels.shape[1], activation='softmax')  # Softmax for multi-class classification\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(X_train, y_train, epochs=20, batch_size=32, validation_data=(X_test, y_test))\n",
        "\n",
        "# Evaluate the model\n",
        "test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
        "print(\"Test Loss:\", test_loss)\n",
        "print(\"Test Accuracy:\", test_accuracy)\n"
      ],
      "metadata": {
        "id": "ejrlHQyku4VL",
        "outputId": "ea91f2e5-6a71-4195-a2ff-07a390126f16",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        }
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'embedding_matrix' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-f4d0f9234724>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Define the LSTM model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m model = Sequential([\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mEmbedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0membedding_matrix\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_sequence_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mBidirectional\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLSTM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_sequences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mDropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'embedding_matrix' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ktS8BIeqKnBX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "search_keyword = input(\"Masukkan kata kunci: \")\n",
        "recommend_podcast(search_keyword, data, tokenizer, model, max_sequence_length)"
      ],
      "metadata": {
        "id": "ZCIX7Ar9T6Ab",
        "outputId": "5baebe36-28bc-434b-e67a-37279c22fe24",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Masukkan kata kunci: afhaf\n",
            "1/1 [==============================] - 0s 187ms/step\n",
            "Genre: arts and entertainment\n",
            "Podcast Name: Tea & Strumpets: A Regency Romance Review\n",
            "Spotify URL: https://open.spotify.com/show/3vIJiD1WQoEv09IRnqo91G\n",
            "\n",
            "Genre: arts and entertainment\n",
            "Podcast Name: The GenreVerse Podcast Network by LRM Online\n",
            "Spotify URL: https://open.spotify.com/show/084cvmiweF4j7gAcN1KN8Q\n",
            "\n",
            "Genre: arts and entertainment\n",
            "Podcast Name: Hood Cash Radio: Podcast Edition\n",
            "Spotify URL: https://open.spotify.com/show/40R8FBPo3zfvBUdyualJHd\n",
            "\n",
            "Genre: arts and entertainment\n",
            "Podcast Name: The Sword and Laser\n",
            "Spotify URL: https://open.spotify.com/show/0VeoMYXPOgXBxSxJBPetRA\n",
            "\n",
            "Genre: arts and entertainment\n",
            "Podcast Name: Genre Geschehen\n",
            "Spotify URL: https://open.spotify.com/show/7mF1YFf8oOKagn769dXrDc\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional\n",
        "\n",
        "# Define the LSTM model\n",
        "model = Sequential([\n",
        "    Embedding(vocab_size, embedding_dim, weights=[embedding_matrix], input_length=max_sequence_length, trainable=False),\n",
        "    Bidirectional(LSTM(128, return_sequences=True)),\n",
        "    Dropout(0.5),\n",
        "    LSTM(64),\n",
        "    Dropout(0.5),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(labels.shape[1], activation='softmax')  # Softmax for multi-class classification\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(X_train, y_train, epochs=20, batch_size=32, validation_data=(X_test, y_test))\n",
        "\n",
        "# Evaluate the model\n",
        "test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
        "print(\"Test Loss:\", test_loss)\n",
        "print(\"Test Accuracy:\", test_accuracy)\n"
      ],
      "metadata": {
        "id": "C5zDf5uMtztq",
        "outputId": "2f2d954e-b6b7-4609-888f-aec666cb7baf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        }
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'embedding_matrix' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-f4d0f9234724>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Define the LSTM model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m model = Sequential([\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mEmbedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0membedding_matrix\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_sequence_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mBidirectional\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLSTM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_sequences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mDropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'embedding_matrix' is not defined"
          ]
        }
      ]
    }
  ]
}