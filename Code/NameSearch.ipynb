{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense, TimeDistributed, Bidirectional, Dropout\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import pad_sequences\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from keras import backend as K\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except RuntimeError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Genre</th>\n",
       "      <th>Podcast Name</th>\n",
       "      <th>Description</th>\n",
       "      <th>Publisher</th>\n",
       "      <th>Total Episodes</th>\n",
       "      <th>Spotify URL</th>\n",
       "      <th>Cover Image URL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>arts and entertainment</td>\n",
       "      <td>Easy Stories in English</td>\n",
       "      <td>Learning a language is hard, but Easy Stories ...</td>\n",
       "      <td>Ariel Goodbody, Polyglot English Teacher &amp; Gla...</td>\n",
       "      <td>216</td>\n",
       "      <td>https://open.spotify.com/show/23zdIqNUb0riR51w...</td>\n",
       "      <td>https://i.scdn.co/image/ab6765630000ba8a767693...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>arts and entertainment</td>\n",
       "      <td>Podcast Buku Kutu</td>\n",
       "      <td>EPISODE BARU SETIAP SENIN, RABU, dan JUMAT -- ...</td>\n",
       "      <td>Aditya Hadi - PODLUCK</td>\n",
       "      <td>162</td>\n",
       "      <td>https://open.spotify.com/show/3w5zKrbQ6kgB0RKI...</td>\n",
       "      <td>https://i.scdn.co/image/ab6765630000ba8a04fa1a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>arts and entertainment</td>\n",
       "      <td>Underwood and Flinch and Other Audiobooks by M...</td>\n",
       "      <td>Underwood and Flinch is a three-time Parsec aw...</td>\n",
       "      <td>Mike Bennett</td>\n",
       "      <td>244</td>\n",
       "      <td>https://open.spotify.com/show/3VwIE3bG0zpTCNzR...</td>\n",
       "      <td>https://i.scdn.co/image/ab6765630000ba8a4e7b42...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>arts and entertainment</td>\n",
       "      <td>Podcast Resensi Buku</td>\n",
       "      <td>Kumpulan resensi beragam buku berbagai genre d...</td>\n",
       "      <td>Podcast Resensi Buku - PODLUCK</td>\n",
       "      <td>264</td>\n",
       "      <td>https://open.spotify.com/show/6woLsDl6CSntzeWU...</td>\n",
       "      <td>https://i.scdn.co/image/ab6765630000ba8a1e97ef...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>arts and entertainment</td>\n",
       "      <td>SupremeMasterTV</td>\n",
       "      <td>Supreme Master Television is an international ...</td>\n",
       "      <td>SupremeMasterTV</td>\n",
       "      <td>500</td>\n",
       "      <td>https://open.spotify.com/show/5bCgERRINgZWhauS...</td>\n",
       "      <td>https://i.scdn.co/image/ab6765630000ba8a7899e5...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Genre                                       Podcast Name  \\\n",
       "0  arts and entertainment                            Easy Stories in English   \n",
       "1  arts and entertainment                                  Podcast Buku Kutu   \n",
       "2  arts and entertainment  Underwood and Flinch and Other Audiobooks by M...   \n",
       "3  arts and entertainment                               Podcast Resensi Buku   \n",
       "4  arts and entertainment                                    SupremeMasterTV   \n",
       "\n",
       "                                         Description  \\\n",
       "0  Learning a language is hard, but Easy Stories ...   \n",
       "1  EPISODE BARU SETIAP SENIN, RABU, dan JUMAT -- ...   \n",
       "2  Underwood and Flinch is a three-time Parsec aw...   \n",
       "3  Kumpulan resensi beragam buku berbagai genre d...   \n",
       "4  Supreme Master Television is an international ...   \n",
       "\n",
       "                                           Publisher  Total Episodes  \\\n",
       "0  Ariel Goodbody, Polyglot English Teacher & Gla...             216   \n",
       "1                              Aditya Hadi - PODLUCK             162   \n",
       "2                                       Mike Bennett             244   \n",
       "3                     Podcast Resensi Buku - PODLUCK             264   \n",
       "4                                    SupremeMasterTV             500   \n",
       "\n",
       "                                         Spotify URL  \\\n",
       "0  https://open.spotify.com/show/23zdIqNUb0riR51w...   \n",
       "1  https://open.spotify.com/show/3w5zKrbQ6kgB0RKI...   \n",
       "2  https://open.spotify.com/show/3VwIE3bG0zpTCNzR...   \n",
       "3  https://open.spotify.com/show/6woLsDl6CSntzeWU...   \n",
       "4  https://open.spotify.com/show/5bCgERRINgZWhauS...   \n",
       "\n",
       "                                     Cover Image URL  \n",
       "0  https://i.scdn.co/image/ab6765630000ba8a767693...  \n",
       "1  https://i.scdn.co/image/ab6765630000ba8a04fa1a...  \n",
       "2  https://i.scdn.co/image/ab6765630000ba8a4e7b42...  \n",
       "3  https://i.scdn.co/image/ab6765630000ba8a1e97ef...  \n",
       "4  https://i.scdn.co/image/ab6765630000ba8a7899e5...  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the dataset\n",
    "file_path = 'podcasts_data.csv'\n",
    "dataset = pd.read_csv(file_path)\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning\n",
    "def clean_text(text):\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n",
    "    text = re.sub(r'\\d+', '', text)  # Remove numbers\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Remove extra whitespace\n",
    "    text = text.strip()  # Remove leading and trailing whitespace\n",
    "    stop_words = set(stopwords.words('english', 'indonesian'))\n",
    "    text = ' '.join(word for word in text.split() if word not in stop_words)  # Remove stop words\n",
    "    return text\n",
    "\n",
    "dataset['Podcast Name'] = dataset['Podcast Name'].apply(clean_text)\n",
    "dataset['Genre'] = dataset['Genre'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with NaN values in 'Podcast Name' column\n",
    "podcast_data = dataset.dropna(subset=['Podcast Name'])\n",
    "\n",
    "# Extract podcast names\n",
    "podcast_names = podcast_data['Podcast Name'].values\n",
    "\n",
    "# Extract relevant columns\n",
    "podcast_names = podcast_data['Podcast Name'].values\n",
    "podcast_genres = podcast_data['Genre'].values\n",
    "podcast_descriptions = podcast_data['Description'].values\n",
    "podcast_publishers = podcast_data['Publisher'].values\n",
    "podcast_spotify_urls = podcast_data['Spotify URL'].values\n",
    "podcast_cover_image_urls = podcast_data['Cover Image URL'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization and Vectorization\n",
    "tokenizer = Tokenizer(oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(podcast_names)\n",
    "\n",
    "# Convert podcast names to sequences of integers\n",
    "sequences = tokenizer.texts_to_sequences(podcast_names)\n",
    "\n",
    "# Pad sequences to have the same length\n",
    "max_length = max(len(seq) for seq in sequences)\n",
    "padded_sequences = pad_sequences(sequences, maxlen=max_length, padding='post')\n",
    "\n",
    "# Get the vocabulary size for the embedding layer\n",
    "vocab_size = len(tokenizer.word_index) + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 23, 128)           1153664   \n",
      "                                                                 \n",
      " bidirectional (Bidirectiona  (None, 23, 128)          98816     \n",
      " l)                                                              \n",
      "                                                                 \n",
      " bidirectional_1 (Bidirectio  (None, 23, 128)          98816     \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 23, 128)           0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 23, 128)           16512     \n",
      "                                                                 \n",
      " time_distributed (TimeDistr  (None, 23, 9013)         1162677   \n",
      " ibuted)                                                         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,530,485\n",
      "Trainable params: 2,530,485\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    Embedding(input_dim=vocab_size, output_dim=128, input_length=max_length),\n",
    "    Bidirectional(LSTM(64, return_sequences=True)),\n",
    "    Bidirectional(LSTM(64, return_sequences=True)),\n",
    "    Dropout(0.5),\n",
    "    Dense(128, activation = 'relu'),\n",
    "    TimeDistributed(Dense(vocab_size, activation='softmax'))\n",
    "])\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "211/211 [==============================] - 7s 23ms/step - loss: 1.7939 - accuracy: 0.8641 - val_loss: 1.0752 - val_accuracy: 0.8736\n",
      "Epoch 2/20\n",
      "211/211 [==============================] - 4s 20ms/step - loss: 0.9900 - accuracy: 0.8765 - val_loss: 1.0087 - val_accuracy: 0.8794\n",
      "Epoch 3/20\n",
      "211/211 [==============================] - 4s 20ms/step - loss: 0.9096 - accuracy: 0.8786 - val_loss: 0.9520 - val_accuracy: 0.8806\n",
      "Epoch 4/20\n",
      "211/211 [==============================] - 4s 20ms/step - loss: 0.8605 - accuracy: 0.8814 - val_loss: 0.9269 - val_accuracy: 0.8835\n",
      "Epoch 5/20\n",
      "211/211 [==============================] - 4s 20ms/step - loss: 0.8212 - accuracy: 0.8835 - val_loss: 0.9090 - val_accuracy: 0.8861\n",
      "Epoch 6/20\n",
      "211/211 [==============================] - 4s 20ms/step - loss: 0.7748 - accuracy: 0.8864 - val_loss: 0.9006 - val_accuracy: 0.8897\n",
      "Epoch 7/20\n",
      "211/211 [==============================] - 4s 20ms/step - loss: 0.7171 - accuracy: 0.8912 - val_loss: 0.8663 - val_accuracy: 0.8928\n",
      "Epoch 8/20\n",
      "211/211 [==============================] - 4s 20ms/step - loss: 0.6618 - accuracy: 0.8951 - val_loss: 0.8277 - val_accuracy: 0.8967\n",
      "Epoch 9/20\n",
      "211/211 [==============================] - 4s 20ms/step - loss: 0.6088 - accuracy: 0.8995 - val_loss: 0.7864 - val_accuracy: 0.9024\n",
      "Epoch 10/20\n",
      "211/211 [==============================] - 4s 20ms/step - loss: 0.5593 - accuracy: 0.9027 - val_loss: 0.7478 - val_accuracy: 0.9048\n",
      "Epoch 11/20\n",
      "211/211 [==============================] - 4s 20ms/step - loss: 0.5193 - accuracy: 0.9054 - val_loss: 0.7185 - val_accuracy: 0.9107\n",
      "Epoch 12/20\n",
      "211/211 [==============================] - 4s 20ms/step - loss: 0.4858 - accuracy: 0.9082 - val_loss: 0.6955 - val_accuracy: 0.9102\n",
      "Epoch 13/20\n",
      "211/211 [==============================] - 4s 20ms/step - loss: 0.4586 - accuracy: 0.9105 - val_loss: 0.6714 - val_accuracy: 0.9152\n",
      "Epoch 14/20\n",
      "211/211 [==============================] - 4s 20ms/step - loss: 0.4341 - accuracy: 0.9127 - val_loss: 0.6491 - val_accuracy: 0.9183\n",
      "Epoch 15/20\n",
      "211/211 [==============================] - 4s 20ms/step - loss: 0.4102 - accuracy: 0.9155 - val_loss: 0.6289 - val_accuracy: 0.9220\n",
      "Epoch 16/20\n",
      "211/211 [==============================] - 4s 20ms/step - loss: 0.3863 - accuracy: 0.9186 - val_loss: 0.6083 - val_accuracy: 0.9262\n",
      "Epoch 17/20\n",
      "211/211 [==============================] - 4s 20ms/step - loss: 0.3602 - accuracy: 0.9223 - val_loss: 0.5856 - val_accuracy: 0.9291\n",
      "Epoch 18/20\n",
      "211/211 [==============================] - 4s 20ms/step - loss: 0.3335 - accuracy: 0.9269 - val_loss: 0.5627 - val_accuracy: 0.9344\n",
      "Epoch 19/20\n",
      "211/211 [==============================] - 4s 20ms/step - loss: 0.3099 - accuracy: 0.9306 - val_loss: 0.5428 - val_accuracy: 0.9381\n",
      "Epoch 20/20\n",
      "211/211 [==============================] - 4s 20ms/step - loss: 0.2854 - accuracy: 0.9346 - val_loss: 0.5206 - val_accuracy: 0.9426\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x265a9116c50>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prepare labels to match the output shape of the model\n",
    "labels = np.expand_dims(padded_sequences, axis=-1)\n",
    "\n",
    "# Train the model\n",
    "model.fit(padded_sequences, labels, epochs=20, batch_size=64, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33/33 [==============================] - 0s 10ms/step\n",
      "33/33 [==============================] - 0s 4ms/step\n",
      "33/33 [==============================] - 0s 4ms/step\n",
      "33/33 [==============================] - 0s 4ms/step\n",
      "33/33 [==============================] - 0s 4ms/step\n",
      "33/33 [==============================] - 0s 4ms/step\n",
      "33/33 [==============================] - 0s 4ms/step\n",
      "33/33 [==============================] - 0s 5ms/step\n",
      "33/33 [==============================] - 0s 4ms/step\n",
      "33/33 [==============================] - 0s 4ms/step\n",
      "33/33 [==============================] - 0s 4ms/step\n",
      "33/33 [==============================] - 0s 4ms/step\n",
      "33/33 [==============================] - 0s 4ms/step\n",
      "33/33 [==============================] - 0s 4ms/step\n",
      "33/33 [==============================] - 0s 4ms/step\n",
      "33/33 [==============================] - 0s 3ms/step\n",
      "3/3 [==============================] - 0s 4ms/step\n"
     ]
    }
   ],
   "source": [
    "def get_podcast_embeddings(model, data, batch_size=1048):\n",
    "    embeddings = []\n",
    "    for i in range(0, len(data), batch_size):\n",
    "        batch_data = data[i:i + batch_size]\n",
    "        batch_embeddings = model.predict(batch_data)\n",
    "        batch_embeddings = batch_embeddings.reshape(batch_embeddings.shape[0], -1)\n",
    "        embeddings.append(batch_embeddings)\n",
    "    embeddings = np.vstack(embeddings)\n",
    "    return embeddings\n",
    "\n",
    "# Prepare the embeddings for the podcasts\n",
    "podcast_embeddings = get_podcast_embeddings(model, padded_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_podcasts(query, top_k=5):\n",
    "    # Tokenize and pad the query\n",
    "    query_sequence = tokenizer.texts_to_sequences([query])\n",
    "    query_padded = pad_sequences(query_sequence, maxlen=max_length, padding='post')\n",
    "\n",
    "    # Encode the query using the trained model\n",
    "    query_embedding = model.predict(query_padded)\n",
    "    query_embedding = query_embedding.reshape(1, -1)\n",
    "    cosine_scores = cosine_similarity(query_embedding, podcast_embeddings)\n",
    "\n",
    "    # Get the top_k similar podcasts\n",
    "    top_k_indices = np.argsort(cosine_scores[0])[-top_k:][::-1]\n",
    "\n",
    "    # Retrieve the corresponding podcast names\n",
    "    similar_podcasts = [{\n",
    "        'Name': podcast_names[idx],\n",
    "        'Genre': podcast_genres[idx],\n",
    "        #'Description': podcast_descriptions[idx],\n",
    "        'Publisher': podcast_publishers[idx],\n",
    "        'Spotify URL': podcast_spotify_urls[idx],\n",
    "        'Cover Image URL': podcast_cover_image_urls[idx]\n",
    "    } for idx in top_k_indices]\n",
    "\n",
    "    return similar_podcasts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 23ms/step\n",
      "[{'Name': 'h', 'Genre': 'comedy', 'Publisher': '14H14', 'Spotify URL': 'https://open.spotify.com/show/2Bm4iMJR4GFpLaR9eTV3jJ', 'Cover Image URL': 'https://i.scdn.co/image/85ac2c94a91a0c0cc99a43b3110098afba8e5045'}, {'Name': 'dmold', 'Genre': 'games', 'Publisher': '2DMOld', 'Spotify URL': 'https://open.spotify.com/show/7bdlE1ubxTsV3lmCibQicH', 'Cover Image URL': 'https://i.scdn.co/image/ab6765630000ba8a6982f8da85982ff104e0428d'}, {'Name': 'lingo', 'Genre': 'business', 'Publisher': 'The Lingo', 'Spotify URL': 'https://open.spotify.com/show/3H6cDSrjo2rGrp9saGaX46', 'Cover Image URL': 'https://i.scdn.co/image/7a795c7afe5740355040d5f81d38998432c7a84e'}, {'Name': 'exit', 'Genre': 'comedy', 'Publisher': \"Stick'n'Poke Productions\", 'Spotify URL': 'https://open.spotify.com/show/7adHI2N3DNDXvQTeCWWZhQ', 'Cover Image URL': 'https://i.scdn.co/image/fab70837c0cf3162e090ec6383e65b01df0c6f21'}, {'Name': 'jancukers', 'Genre': 'language', 'Publisher': 'jembleng edan', 'Spotify URL': 'https://open.spotify.com/show/4HGaBGsRYuM0wv1IStkQ3t', 'Cover Image URL': 'https://i.scdn.co/image/29bf3f6d623df6d6549646ae7bdbedcf9ddc7149'}]\n"
     ]
    }
   ],
   "source": [
    "# Example search\n",
    "query = \"musik\"\n",
    "similar_podcasts = search_podcasts(query)\n",
    "print(similar_podcasts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "import json\n",
    "model = load_model(\"model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_json = model.to_json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
