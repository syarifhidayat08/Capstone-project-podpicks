{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Capstone\\.conda\\lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:11: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\syari\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense, TimeDistributed, Bidirectional, Dropout\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import pad_sequences\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.utils import shuffle\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from keras import backend as K\n",
    "nltk.download('stopwords')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text Cleaning Function\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    This function cleans text by removing punctuation, numbers, extra whitespace, \n",
    "    and stopwords.\n",
    "    \"\"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    text = text.strip()\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    text = ' '.join(word for word in text.split() if word not in stop_words)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and Preprocess Data \n",
    "dataset = pd.read_csv('../Data/podcasts_data.csv')\n",
    "dataset = shuffle(dataset)  \n",
    "dataset['Podcast Name'] = dataset['Podcast Name'].apply(clean_text)\n",
    "dataset['Genre'] = dataset['Genre'].apply(clean_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with NaN values in 'Podcast Name' column\n",
    "podcast_data = dataset.dropna(subset=['Podcast Name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract relevant columns\n",
    "podcast_names = podcast_data['Podcast Name'].values\n",
    "podcast_descriptions = podcast_data['Description'].values\n",
    "podcast_publishers = podcast_data['Publisher'].values\n",
    "podcast_spotify_urls = podcast_data['Spotify URL'].values\n",
    "podcast_cover_image_urls = podcast_data['Cover Image URL'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization and Vectorization\n",
    "tokenizer = Tokenizer(oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(podcast_names)\n",
    "sequences = tokenizer.texts_to_sequences(podcast_names)\n",
    "max_length = max(len(seq) for seq in sequences)\n",
    "padded_sequences = pad_sequences(sequences, maxlen=max_length, padding='post')\n",
    "vocab_size = len(tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Capstone\\.conda\\lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Sentence-BERT Embedding\n",
    "bert_model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
    "podcast_embeddings = bert_model.encode(podcast_names, convert_to_tensor=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (None, 23, 128)           1153664   \n",
      "                                                                 \n",
      " bidirectional_2 (Bidirectio  (None, 23, 128)          98816     \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 23, 128)           0         \n",
      "                                                                 \n",
      " bidirectional_3 (Bidirectio  (None, 23, 128)          98816     \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 23, 128)           0         \n",
      "                                                                 \n",
      " time_distributed_1 (TimeDis  (None, 23, 9013)         1162677   \n",
      " tributed)                                                       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,513,973\n",
      "Trainable params: 2,513,973\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    Embedding(input_dim=vocab_size, output_dim=128, input_length=max_length),\n",
    "    Bidirectional(LSTM(64, return_sequences=True)),\n",
    "    Dropout(0.5),\n",
    "    Bidirectional(LSTM(64, return_sequences=True)),\n",
    "    Dropout(0.5),\n",
    "    TimeDistributed(Dense(vocab_size, activation='softmax'))\n",
    "])\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "106/106 [==============================] - 22s 116ms/step - loss: 2.8988 - accuracy: 0.8606 - val_loss: 1.1317 - val_accuracy: 0.8708\n",
      "Epoch 2/30\n",
      "106/106 [==============================] - 11s 101ms/step - loss: 1.0956 - accuracy: 0.8706 - val_loss: 1.0525 - val_accuracy: 0.8708\n",
      "Epoch 3/30\n",
      "106/106 [==============================] - 11s 108ms/step - loss: 1.0348 - accuracy: 0.8731 - val_loss: 1.0306 - val_accuracy: 0.8767\n",
      "Epoch 4/30\n",
      "106/106 [==============================] - 11s 101ms/step - loss: 1.0140 - accuracy: 0.8758 - val_loss: 1.0174 - val_accuracy: 0.8777\n",
      "Epoch 5/30\n",
      "106/106 [==============================] - 11s 101ms/step - loss: 0.9960 - accuracy: 0.8769 - val_loss: 0.9990 - val_accuracy: 0.8788\n",
      "Epoch 6/30\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 0.9724 - accuracy: 0.8781 - val_loss: 0.9746 - val_accuracy: 0.8792\n",
      "Epoch 7/30\n",
      "106/106 [==============================] - 11s 105ms/step - loss: 0.9453 - accuracy: 0.8787 - val_loss: 0.9492 - val_accuracy: 0.8796\n",
      "Epoch 8/30\n",
      "106/106 [==============================] - 12s 109ms/step - loss: 0.9193 - accuracy: 0.8793 - val_loss: 0.9250 - val_accuracy: 0.8797\n",
      "Epoch 9/30\n",
      "106/106 [==============================] - 11s 103ms/step - loss: 0.8941 - accuracy: 0.8802 - val_loss: 0.8996 - val_accuracy: 0.8817\n",
      "Epoch 10/30\n",
      "106/106 [==============================] - 12s 109ms/step - loss: 0.8671 - accuracy: 0.8812 - val_loss: 0.8723 - val_accuracy: 0.8822\n",
      "Epoch 11/30\n",
      "106/106 [==============================] - 11s 105ms/step - loss: 0.8412 - accuracy: 0.8820 - val_loss: 0.8486 - val_accuracy: 0.8842\n",
      "Epoch 12/30\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 0.8190 - accuracy: 0.8827 - val_loss: 0.8296 - val_accuracy: 0.8847\n",
      "Epoch 13/30\n",
      "106/106 [==============================] - 11s 105ms/step - loss: 0.7992 - accuracy: 0.8837 - val_loss: 0.8132 - val_accuracy: 0.8854\n",
      "Epoch 14/30\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 0.7818 - accuracy: 0.8843 - val_loss: 0.7984 - val_accuracy: 0.8864\n",
      "Epoch 15/30\n",
      "106/106 [==============================] - 12s 109ms/step - loss: 0.7654 - accuracy: 0.8851 - val_loss: 0.7847 - val_accuracy: 0.8890\n",
      "Epoch 16/30\n",
      "106/106 [==============================] - 12s 111ms/step - loss: 0.7498 - accuracy: 0.8862 - val_loss: 0.7723 - val_accuracy: 0.8890\n",
      "Epoch 17/30\n",
      "106/106 [==============================] - 10s 98ms/step - loss: 0.7347 - accuracy: 0.8872 - val_loss: 0.7600 - val_accuracy: 0.8897\n",
      "Epoch 18/30\n",
      "106/106 [==============================] - 10s 96ms/step - loss: 0.7209 - accuracy: 0.8879 - val_loss: 0.7484 - val_accuracy: 0.8916\n",
      "Epoch 19/30\n",
      "106/106 [==============================] - 11s 105ms/step - loss: 0.7067 - accuracy: 0.8887 - val_loss: 0.7380 - val_accuracy: 0.8927\n",
      "Epoch 20/30\n",
      "106/106 [==============================] - 12s 114ms/step - loss: 0.6940 - accuracy: 0.8895 - val_loss: 0.7268 - val_accuracy: 0.8939\n",
      "Epoch 21/30\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 0.6804 - accuracy: 0.8906 - val_loss: 0.7165 - val_accuracy: 0.8950\n",
      "Epoch 22/30\n",
      "106/106 [==============================] - 8s 76ms/step - loss: 0.6676 - accuracy: 0.8916 - val_loss: 0.7057 - val_accuracy: 0.8963\n",
      "Epoch 23/30\n",
      "106/106 [==============================] - 8s 75ms/step - loss: 0.6546 - accuracy: 0.8924 - val_loss: 0.6950 - val_accuracy: 0.8979\n",
      "Epoch 24/30\n",
      "106/106 [==============================] - 8s 78ms/step - loss: 0.6417 - accuracy: 0.8937 - val_loss: 0.6853 - val_accuracy: 0.8995\n",
      "Epoch 25/30\n",
      "106/106 [==============================] - 9s 85ms/step - loss: 0.6298 - accuracy: 0.8946 - val_loss: 0.6752 - val_accuracy: 0.9020\n",
      "Epoch 26/30\n",
      "106/106 [==============================] - 11s 106ms/step - loss: 0.6170 - accuracy: 0.8957 - val_loss: 0.6642 - val_accuracy: 0.9025\n",
      "Epoch 27/30\n",
      "106/106 [==============================] - 11s 102ms/step - loss: 0.6047 - accuracy: 0.8968 - val_loss: 0.6541 - val_accuracy: 0.9041\n",
      "Epoch 28/30\n",
      "106/106 [==============================] - 11s 102ms/step - loss: 0.5936 - accuracy: 0.8978 - val_loss: 0.6447 - val_accuracy: 0.9057\n",
      "Epoch 29/30\n",
      "106/106 [==============================] - 11s 102ms/step - loss: 0.5814 - accuracy: 0.8992 - val_loss: 0.6347 - val_accuracy: 0.9068\n",
      "Epoch 30/30\n",
      "106/106 [==============================] - 11s 102ms/step - loss: 0.5708 - accuracy: 0.9003 - val_loss: 0.6254 - val_accuracy: 0.9086\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2a283839b10>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prepare labels to match the output shape of the model\n",
    "labels = np.expand_dims(padded_sequences, axis=-1)\n",
    "\n",
    "# Train the model\n",
    "model.fit(padded_sequences, labels, epochs=30, batch_size=128, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved succesfully\n"
     ]
    }
   ],
   "source": [
    "model.save('model.h5')\n",
    "print('Model saved succesfully')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
