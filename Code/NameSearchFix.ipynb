{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Capstone\\.conda\\lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:11: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense, TimeDistributed, Bidirectional, Dropout\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import pad_sequences\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Genre</th>\n",
       "      <th>Podcast Name</th>\n",
       "      <th>Description</th>\n",
       "      <th>Publisher</th>\n",
       "      <th>Total Episodes</th>\n",
       "      <th>Spotify URL</th>\n",
       "      <th>Cover Image URL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>arts and entertainment</td>\n",
       "      <td>Easy Stories in English</td>\n",
       "      <td>Learning a language is hard, but Easy Stories ...</td>\n",
       "      <td>Ariel Goodbody, Polyglot English Teacher &amp; Gla...</td>\n",
       "      <td>216</td>\n",
       "      <td>https://open.spotify.com/show/23zdIqNUb0riR51w...</td>\n",
       "      <td>https://i.scdn.co/image/ab6765630000ba8a767693...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>arts and entertainment</td>\n",
       "      <td>Podcast Buku Kutu</td>\n",
       "      <td>EPISODE BARU SETIAP SENIN, RABU, dan JUMAT -- ...</td>\n",
       "      <td>Aditya Hadi - PODLUCK</td>\n",
       "      <td>162</td>\n",
       "      <td>https://open.spotify.com/show/3w5zKrbQ6kgB0RKI...</td>\n",
       "      <td>https://i.scdn.co/image/ab6765630000ba8a04fa1a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>arts and entertainment</td>\n",
       "      <td>Underwood and Flinch and Other Audiobooks by M...</td>\n",
       "      <td>Underwood and Flinch is a three-time Parsec aw...</td>\n",
       "      <td>Mike Bennett</td>\n",
       "      <td>244</td>\n",
       "      <td>https://open.spotify.com/show/3VwIE3bG0zpTCNzR...</td>\n",
       "      <td>https://i.scdn.co/image/ab6765630000ba8a4e7b42...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>arts and entertainment</td>\n",
       "      <td>Podcast Resensi Buku</td>\n",
       "      <td>Kumpulan resensi beragam buku berbagai genre d...</td>\n",
       "      <td>Podcast Resensi Buku - PODLUCK</td>\n",
       "      <td>264</td>\n",
       "      <td>https://open.spotify.com/show/6woLsDl6CSntzeWU...</td>\n",
       "      <td>https://i.scdn.co/image/ab6765630000ba8a1e97ef...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>arts and entertainment</td>\n",
       "      <td>SupremeMasterTV</td>\n",
       "      <td>Supreme Master Television is an international ...</td>\n",
       "      <td>SupremeMasterTV</td>\n",
       "      <td>500</td>\n",
       "      <td>https://open.spotify.com/show/5bCgERRINgZWhauS...</td>\n",
       "      <td>https://i.scdn.co/image/ab6765630000ba8a7899e5...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Genre                                       Podcast Name  \\\n",
       "0  arts and entertainment                            Easy Stories in English   \n",
       "1  arts and entertainment                                  Podcast Buku Kutu   \n",
       "2  arts and entertainment  Underwood and Flinch and Other Audiobooks by M...   \n",
       "3  arts and entertainment                               Podcast Resensi Buku   \n",
       "4  arts and entertainment                                    SupremeMasterTV   \n",
       "\n",
       "                                         Description  \\\n",
       "0  Learning a language is hard, but Easy Stories ...   \n",
       "1  EPISODE BARU SETIAP SENIN, RABU, dan JUMAT -- ...   \n",
       "2  Underwood and Flinch is a three-time Parsec aw...   \n",
       "3  Kumpulan resensi beragam buku berbagai genre d...   \n",
       "4  Supreme Master Television is an international ...   \n",
       "\n",
       "                                           Publisher  Total Episodes  \\\n",
       "0  Ariel Goodbody, Polyglot English Teacher & Gla...             216   \n",
       "1                              Aditya Hadi - PODLUCK             162   \n",
       "2                                       Mike Bennett             244   \n",
       "3                     Podcast Resensi Buku - PODLUCK             264   \n",
       "4                                    SupremeMasterTV             500   \n",
       "\n",
       "                                         Spotify URL  \\\n",
       "0  https://open.spotify.com/show/23zdIqNUb0riR51w...   \n",
       "1  https://open.spotify.com/show/3w5zKrbQ6kgB0RKI...   \n",
       "2  https://open.spotify.com/show/3VwIE3bG0zpTCNzR...   \n",
       "3  https://open.spotify.com/show/6woLsDl6CSntzeWU...   \n",
       "4  https://open.spotify.com/show/5bCgERRINgZWhauS...   \n",
       "\n",
       "                                     Cover Image URL  \n",
       "0  https://i.scdn.co/image/ab6765630000ba8a767693...  \n",
       "1  https://i.scdn.co/image/ab6765630000ba8a04fa1a...  \n",
       "2  https://i.scdn.co/image/ab6765630000ba8a4e7b42...  \n",
       "3  https://i.scdn.co/image/ab6765630000ba8a1e97ef...  \n",
       "4  https://i.scdn.co/image/ab6765630000ba8a7899e5...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the dataset\n",
    "file_path = '../Data/podcasts_data.csv'\n",
    "dataset = pd.read_csv(file_path)\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Drop rows with NaN values in 'Podcast Name' column\n",
    "podcast_data = dataset.dropna(subset=['Podcast Name'])\n",
    "\n",
    "# Extract podcast names\n",
    "podcast_names = podcast_data['Podcast Name'].values\n",
    "\n",
    "# Extract relevant columns\n",
    "podcast_names = podcast_data['Podcast Name'].values\n",
    "podcast_descriptions = podcast_data['Description'].values\n",
    "podcast_publishers = podcast_data['Publisher'].values\n",
    "podcast_spotify_urls = podcast_data['Spotify URL'].values\n",
    "podcast_cover_image_urls = podcast_data['Cover Image URL'].values\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization and Vectorization\n",
    "tokenizer = Tokenizer(oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(podcast_names)\n",
    "\n",
    "# Convert podcast names to sequences of integers\n",
    "sequences = tokenizer.texts_to_sequences(podcast_names)\n",
    "\n",
    "# Pad sequences to have the same length\n",
    "max_length = max(len(seq) for seq in sequences)\n",
    "padded_sequences = pad_sequences(sequences, maxlen=max_length, padding='post')\n",
    "\n",
    "# Get the vocabulary size for the embedding layer\n",
    "vocab_size = len(tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 23, 128)           1199488   \n",
      "                                                                 \n",
      " bidirectional (Bidirectiona  (None, 23, 128)          98816     \n",
      " l)                                                              \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 23, 128)           0         \n",
      "                                                                 \n",
      " bidirectional_1 (Bidirectio  (None, 23, 128)          98816     \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 23, 128)           0         \n",
      "                                                                 \n",
      " time_distributed (TimeDistr  (None, 23, 9371)         1208859   \n",
      " ibuted)                                                         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,605,979\n",
      "Trainable params: 2,605,979\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Build the model\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=vocab_size, output_dim=128, input_length=max_length),\n",
    "    Bidirectional(LSTM(64, return_sequences=True)),\n",
    "    Dropout(0.5),\n",
    "    Bidirectional(LSTM(64, return_sequences=True)),\n",
    "    Dropout(0.5),\n",
    "    TimeDistributed(Dense(vocab_size, activation='softmax'))\n",
    "])\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "211/211 [==============================] - 32s 109ms/step - loss: 2.2293 - accuracy: 0.8288 - val_loss: 1.2833 - val_accuracy: 0.8463\n",
      "Epoch 2/20\n",
      "211/211 [==============================] - 20s 93ms/step - loss: 1.2395 - accuracy: 0.8456 - val_loss: 1.2534 - val_accuracy: 0.8541\n",
      "Epoch 3/20\n",
      "211/211 [==============================] - 20s 96ms/step - loss: 1.1801 - accuracy: 0.8511 - val_loss: 1.1771 - val_accuracy: 0.8569\n",
      "Epoch 4/20\n",
      "211/211 [==============================] - 20s 96ms/step - loss: 1.0814 - accuracy: 0.8538 - val_loss: 1.0931 - val_accuracy: 0.8581\n",
      "Epoch 5/20\n",
      "211/211 [==============================] - 18s 84ms/step - loss: 1.0111 - accuracy: 0.8565 - val_loss: 1.0419 - val_accuracy: 0.8616\n",
      "Epoch 6/20\n",
      "211/211 [==============================] - 16s 78ms/step - loss: 0.9559 - accuracy: 0.8600 - val_loss: 1.0008 - val_accuracy: 0.8641\n",
      "Epoch 7/20\n",
      "211/211 [==============================] - 16s 77ms/step - loss: 0.9107 - accuracy: 0.8630 - val_loss: 0.9688 - val_accuracy: 0.8678\n",
      "Epoch 8/20\n",
      "211/211 [==============================] - 16s 77ms/step - loss: 0.8722 - accuracy: 0.8659 - val_loss: 0.9437 - val_accuracy: 0.8718\n",
      "Epoch 9/20\n",
      "211/211 [==============================] - 16s 78ms/step - loss: 0.8388 - accuracy: 0.8681 - val_loss: 0.9243 - val_accuracy: 0.8737\n",
      "Epoch 10/20\n",
      "211/211 [==============================] - 19s 89ms/step - loss: 0.8081 - accuracy: 0.8707 - val_loss: 0.9070 - val_accuracy: 0.8754\n",
      "Epoch 11/20\n",
      "211/211 [==============================] - 16s 77ms/step - loss: 0.7783 - accuracy: 0.8728 - val_loss: 0.8881 - val_accuracy: 0.8780\n",
      "Epoch 12/20\n",
      "211/211 [==============================] - 16s 77ms/step - loss: 0.7491 - accuracy: 0.8753 - val_loss: 0.8721 - val_accuracy: 0.8806\n",
      "Epoch 13/20\n",
      "211/211 [==============================] - 16s 77ms/step - loss: 0.7207 - accuracy: 0.8785 - val_loss: 0.8537 - val_accuracy: 0.8841\n",
      "Epoch 14/20\n",
      "211/211 [==============================] - 16s 77ms/step - loss: 0.6922 - accuracy: 0.8818 - val_loss: 0.8338 - val_accuracy: 0.8880\n",
      "Epoch 15/20\n",
      "211/211 [==============================] - 14s 64ms/step - loss: 0.6663 - accuracy: 0.8846 - val_loss: 0.8153 - val_accuracy: 0.8924\n",
      "Epoch 16/20\n",
      "211/211 [==============================] - 16s 74ms/step - loss: 0.6403 - accuracy: 0.8882 - val_loss: 0.8011 - val_accuracy: 0.8950\n",
      "Epoch 17/20\n",
      "211/211 [==============================] - 16s 77ms/step - loss: 0.6154 - accuracy: 0.8917 - val_loss: 0.7827 - val_accuracy: 0.8994\n",
      "Epoch 18/20\n",
      "211/211 [==============================] - 16s 77ms/step - loss: 0.5915 - accuracy: 0.8955 - val_loss: 0.7645 - val_accuracy: 0.9037\n",
      "Epoch 19/20\n",
      "211/211 [==============================] - 17s 82ms/step - loss: 0.5681 - accuracy: 0.8987 - val_loss: 0.7487 - val_accuracy: 0.9068\n",
      "Epoch 20/20\n",
      "211/211 [==============================] - 16s 76ms/step - loss: 0.5466 - accuracy: 0.9019 - val_loss: 0.7349 - val_accuracy: 0.9092\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x16d2739b8e0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prepare labels to match the output shape of the model\n",
    "labels = np.expand_dims(padded_sequences, axis=-1)\n",
    "\n",
    "# Train the model\n",
    "model.fit(padded_sequences, labels, epochs=20, batch_size=64, validation_split=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Capstone\\.conda\\lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Load pre-trained Sentence-BERT model\n",
    "bert_model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
    "\n",
    "# Encode podcast names\n",
    "podcast_embeddings = bert_model.encode(podcast_names, convert_to_tensor=True)\n",
    "\n",
    "def search_podcasts(query, top_k=5):\n",
    "    query_embedding = bert_model.encode([query], convert_to_tensor=True)\n",
    "    cosine_scores = cosine_similarity(query_embedding, podcast_embeddings)\n",
    "\n",
    "    # Get the top_k similar podcasts\n",
    "    top_k_indices = np.argsort(cosine_scores[0])[-top_k:][::-1]\n",
    "\n",
    "    # Retrieve the corresponding podcast names\n",
    "    similar_podcasts = [{\n",
    "        'Name': podcast_names[idx],\n",
    "        'Description': podcast_descriptions[idx],\n",
    "        'Publisher': podcast_publishers[idx],\n",
    "        'Spotify URL': podcast_spotify_urls[idx],\n",
    "        'Cover Image URL': podcast_cover_image_urls[idx]\n",
    "    } for idx in top_k_indices]\n",
    "\n",
    "    return similar_podcasts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'Name': 'Creepy Cooking Staff', 'Description': 'You can’t have a main course without getting a little experimental in the kitchen. The Creepy Cooking Staff is a podcast where UCA fixture Allen Chaney and his co-host Mike Macdee use the ‘ingredients’ supplied by listeners and their guests to try and make something tasty (or at the very least edible). Once a month Allen and Mike tackle the popular tropes and common story types of Creepypasta as suggested to them and try to brainstorm a story on the fly with the help of a guest. The results serve as a humorous glance at attempting to defy a genre in an attempt to improve it. Also fart jokes.', 'Publisher': 'Allen Chaney & Creative Horror', 'Spotify URL': 'https://open.spotify.com/show/7MdFr3sB77ifNwBBB7piaU', 'Cover Image URL': 'https://i.scdn.co/image/6562ccdff6aea29b94775336005bed48347dbd1e'}, {'Name': 'Creepy Cooking Staff', 'Description': 'You can’t have a main course without getting a little experimental in the kitchen. The Creepy Cooking Staff is a podcast where UCA fixture Allen Chaney and his co-host Mike Macdee use the ‘ingredients’ supplied by listeners and their guests to try and make something tasty (or at the very least edible). Once a month Allen and Mike tackle the popular tropes and common story types of Creepypasta as suggested to them and try to brainstorm a story on the fly with the help of a guest. The results serve as a humorous glance at attempting to defy a genre in an attempt to improve it. Also fart jokes.', 'Publisher': 'Allen Chaney & Creative Horror', 'Spotify URL': 'https://open.spotify.com/show/7MdFr3sB77ifNwBBB7piaU', 'Cover Image URL': 'https://i.scdn.co/image/6562ccdff6aea29b94775336005bed48347dbd1e'}, {'Name': 'Creepy Cooking Staff', 'Description': 'You can’t have a main course without getting a little experimental in the kitchen. The Creepy Cooking Staff is a podcast where UCA fixture Allen Chaney and his co-host Mike Macdee use the ‘ingredients’ supplied by listeners and their guests to try and make something tasty (or at the very least edible). Once a month Allen and Mike tackle the popular tropes and common story types of Creepypasta as suggested to them and try to brainstorm a story on the fly with the help of a guest. The results serve as a humorous glance at attempting to defy a genre in an attempt to improve it. Also fart jokes.', 'Publisher': 'Allen Chaney & Creative Horror', 'Spotify URL': 'https://open.spotify.com/show/7MdFr3sB77ifNwBBB7piaU', 'Cover Image URL': 'https://i.scdn.co/image/6562ccdff6aea29b94775336005bed48347dbd1e'}, {'Name': 'BREAKING BREAD w/ CHEF LIZETTE', 'Description': 'Breaking Bread with Chef Lizette is a podcast that underscores the values and principles of putting the customer first, last and always. The customer represents two groups, the “internal customer” & “external customer”. Chef Lizette ignites thoughtful conversations with the worlds finest leaders from all genres to showcase the importance of over delivering on the guest experience. With 3 decades of experience in culinary arts & hospitality leadership Chef Lizette understands that your only as good as your last meal. Please join us …', 'Publisher': 'Chef Lizette', 'Spotify URL': 'https://open.spotify.com/show/64inClSGRhK0CdJogB1YQZ', 'Cover Image URL': 'https://i.scdn.co/image/ab6765630000ba8ace42a6b9823877b2b24e078a'}, {'Name': 'A Yeasted Show with Chef Hensarino', 'Description': 'Chill out with Chef Hensarino while he and the occasional guest discuss anything and everything food and cooking related in addition to ongoing projects. The topics will also dip into other genres of interest such as music & much more at times, with the focus on it being a good vibe, as well as educational or at the very least interesting!', 'Publisher': 'Thomas Hensarino', 'Spotify URL': 'https://open.spotify.com/show/0Mt5gaxAFwdBOZUlJsrpxn', 'Cover Image URL': 'https://i.scdn.co/image/ab6765630000ba8a1adae047cf7d13d9fa8bdaa6'}]\n"
     ]
    }
   ],
   "source": [
    "# Example search\n",
    "query = \"cook\"\n",
    "similar_podcasts = search_podcasts(query)\n",
    "print(similar_podcasts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved succesfully\n"
     ]
    }
   ],
   "source": [
    "model.save('model.h5')\n",
    "print('Model saved succesfully')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
